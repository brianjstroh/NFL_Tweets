{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code chunk was run in R before I realized I should be extracting data in Python first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine csv files\n",
    "\n",
    "# all_place <- readLines(\"place_only.csv\", skipNul = T)\n",
    "# all_place <- c(all_place, readLines(\"place_only_2.csv\", skipNul = T))\n",
    "# all_place <- c(all_place, readLines(\"place_only_3.csv\", skipNul = T))\n",
    "# length(all_place) = 740165\n",
    "# \n",
    "# all_text <- readLines(\"text_only.csv\")\n",
    "# all_text <- c(all_text, readLines(\"text_only_2.csv\"))\n",
    "# all_text <- c(all_text, readLines(\"text_only_3.csv\"))\n",
    "# length(all_text) = 1792511\n",
    "\n",
    "#I had a feeling that the Python script may not capture all places and text in a fashion that lines up.\n",
    "#This is why I captured the full tweet data!\n",
    "#The only problem is that the full tweet data is stored in 18 GBs worth of data, and that I can't figure out how to read it back in as a tweet.\n",
    "#Since it's very easy to split large files on Windows using Git Bash (https://gitforwindows.org/), I did so for each of my tweet files.\n",
    "#This is the command that was used on each of the full_tweets#.csv files: \"split full_tweets_#.csv tweets# -l 10000 -d\"\n",
    "#This means that all but the last files in each split will have 10000 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy for easy array accumulation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cardinals',\n",
       " 'Falcons',\n",
       " 'Ravens',\n",
       " 'Bills',\n",
       " 'Panthers',\n",
       " 'Bears',\n",
       " 'Bengals',\n",
       " 'Browns',\n",
       " 'Cowboys',\n",
       " 'Broncos',\n",
       " 'Lions',\n",
       " 'Packers',\n",
       " 'Texans',\n",
       " 'Colts',\n",
       " 'Jaguars',\n",
       " 'Chiefs',\n",
       " 'Chargers',\n",
       " 'Rams',\n",
       " 'Dolphins',\n",
       " 'Vikings',\n",
       " 'Patriots',\n",
       " 'Saints',\n",
       " 'Giants',\n",
       " 'Jets',\n",
       " 'Raiders',\n",
       " 'Eagles',\n",
       " 'Steelers',\n",
       " '49ers',\n",
       " 'Seahawks',\n",
       " 'Buccaneers',\n",
       " 'Titans',\n",
       " 'Redskins']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in list of NFL teams\n",
    "with open('teams.txt', encoding='utf-8-sig') as team_file:\n",
    "    teams = [line.strip() for line in team_file]\n",
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize array for team mention counts to be aggregated in\n",
    "team_counts = np.zeros((32,), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the list of split tweet files.\n",
    "#I broke these up because I don't think my computer would know what to do when trying to load 18GB into RAM.\n",
    "#Using utf-8-sig encoding because when I saved as a UTF-8 encoded file, it saved it as UTF-8-BOM for some reason.\n",
    "with open(\"file_names.txt\", encoding = \"utf-8-sig\") as file_names_file:\n",
    "        file_names = [line.strip() for line in file_names_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6000  8643 32814 13527  6889  9133 26183 29353 19491 14777 13067 13439\n",
      "  7734  9305  4189 35295  9735 11517  9878 12614 69738 31115 19026  8459\n",
      "  7645 38145 25622 43992 13040  4571 16711  7725]\n"
     ]
    }
   ],
   "source": [
    "#This chews through each of the 1/4 GB files until all 18 GBs are processed.\n",
    "#The list comprehension that counts mentions for each team is pretty quick - just a few seconds for each file once it's read in.\n",
    "for file in file_names:\n",
    "    with open(file, encoding = \"utf-8-sig\") as curr_tweets:\n",
    "        tweets = curr_tweets.readlines()\n",
    "    team_counts += np.array([sum([curr_team in curr_tweet for curr_tweet in tweets]) for curr_team in teams])\n",
    "print(team_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>teams</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Jaguars</td>\n",
       "      <td>4189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Buccaneers</td>\n",
       "      <td>4571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cardinals</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Panthers</td>\n",
       "      <td>6889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Raiders</td>\n",
       "      <td>7645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Redskins</td>\n",
       "      <td>7725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Texans</td>\n",
       "      <td>7734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Jets</td>\n",
       "      <td>8459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Falcons</td>\n",
       "      <td>8643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bears</td>\n",
       "      <td>9133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Colts</td>\n",
       "      <td>9305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chargers</td>\n",
       "      <td>9735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dolphins</td>\n",
       "      <td>9878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rams</td>\n",
       "      <td>11517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Vikings</td>\n",
       "      <td>12614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Seahawks</td>\n",
       "      <td>13040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lions</td>\n",
       "      <td>13067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Packers</td>\n",
       "      <td>13439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bills</td>\n",
       "      <td>13527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Broncos</td>\n",
       "      <td>14777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Titans</td>\n",
       "      <td>16711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Giants</td>\n",
       "      <td>19026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cowboys</td>\n",
       "      <td>19491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Steelers</td>\n",
       "      <td>25622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bengals</td>\n",
       "      <td>26183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Browns</td>\n",
       "      <td>29353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Saints</td>\n",
       "      <td>31115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ravens</td>\n",
       "      <td>32814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Chiefs</td>\n",
       "      <td>35295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Eagles</td>\n",
       "      <td>38145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>49ers</td>\n",
       "      <td>43992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Patriots</td>\n",
       "      <td>69738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         teams  mentions\n",
       "14     Jaguars      4189\n",
       "29  Buccaneers      4571\n",
       "0    Cardinals      6000\n",
       "4     Panthers      6889\n",
       "24     Raiders      7645\n",
       "31    Redskins      7725\n",
       "12      Texans      7734\n",
       "23        Jets      8459\n",
       "1      Falcons      8643\n",
       "5        Bears      9133\n",
       "13       Colts      9305\n",
       "16    Chargers      9735\n",
       "18    Dolphins      9878\n",
       "17        Rams     11517\n",
       "19     Vikings     12614\n",
       "28    Seahawks     13040\n",
       "10       Lions     13067\n",
       "11     Packers     13439\n",
       "3        Bills     13527\n",
       "9      Broncos     14777\n",
       "30      Titans     16711\n",
       "22      Giants     19026\n",
       "8      Cowboys     19491\n",
       "26    Steelers     25622\n",
       "6      Bengals     26183\n",
       "7       Browns     29353\n",
       "21      Saints     31115\n",
       "2       Ravens     32814\n",
       "15      Chiefs     35295\n",
       "25      Eagles     38145\n",
       "27       49ers     43992\n",
       "20    Patriots     69738"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfl_df = pd.DataFrame({'teams':teams, 'mentions':team_counts})\n",
    "nfl_df.sort_values(by = ['mentions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of team mention counts above makes sense to me, but also I'm realizing now that most of the statistical assumptions necessary for the Chi-Squared test for equal proportions are violated. These mentions on twitter certainly are not independent, as most of them likely stem from specific actions that occured recently. The independence assumption is definitely violated when retweets are added into the mix. \n",
    "\n",
    "I don't believe that the assumption of random sampling is satisfied either, as we are just looking at a snapshot of the season, and these mentions could be skewed due to events that unfolded in just this past weekend of football. However, this can be addressed by limiting the scope of the analysis to just the most popular team in the NFL at the present moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_df.to_csv(\"nfl_team_mentions.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
